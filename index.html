<!doctype html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Policy Agnostic RL</title>
		<link rel="stylesheet" href="styles.css" />
		<script src="https://cdn.tailwindcss.com"></script>
		<link
			rel="stylesheet"
			href="https://cdnjs.cloudflare.com/ajax/libs/fullPage.js/4.0.20/fullpage.min.css"
		/>
	</head>
	<body class="font-sans">
		<div id="fullpage">
			<div class="section flex items-center justify-center h-screen bg-white">
				<div
					class="intro text-center p-4 max-w-screen-md mx-auto overflow-visible"
				>
					<h1 class="text-4xl md:text-6xl font-bold mb-4 leading-tight">
						Policy Agnostic RL
					</h1>
					<h2 class="text-2xl md:text-4xl font-medium mb-6">
						Fine-Tuning Multiple Policy Classes with Actor-Critic RL
					</h2>
					<p class="text-lg md:text-xl mb-6">
						Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar,
						Archit Sharma, Chelsea Finn, Aviral Kumar
					</p>
					<img
						src="./data/figure_1.jpeg"
						alt="Figure 1"
						class="figure mx-auto max-w-full h-auto rounded-lg shadow-lg"
					/>
				</div>
			</div>
			<div class="section">
				<div class="motivation_rl">
					<h2 class="text-3xl md:text-4xl font-bold mb-4">
						Motivation - why RL?
					</h2>
					<p>
						The best performing models for robotics tasks - large pre-trained
						transformers, diffusion policies - still perform poorly on new tasks
						or slightly unseen conditions. <b>Sample-efficient RL</b> is a
						recipe for improving a policyâ€™s performance cheaply and fast.
					</p>
					<p>
						The <b>Actor-Critic RL</b> architecture yields fast improvement, but
						has only been used so far with small, gaussian policies. Can we get
						the best of both worlds, and improve
						<i>large pre-trained transformers</i> or
						<i>diffusion policies</i> through interaction?
					</p>
				</div>
			</div>
			<div class="section p-6 md:p-12 bg-white">
				<div class="motivation_actor_critic max-w-3xl mx-auto">
					<h2 class="text-3xl md:text-4xl font-bold mb-4 text-center">
						What is the challenge?
					</h2>
					<p class="text-base md:text-lg mb-4">
						The Critic objective will generally look something like:
					</p>
					<div class="overflow-x-auto">
						<p class="font-mono bg-gray-100 p-2 text-base rounded mb-4">
							\[ \mathcal{L}^Q = \mathbb{E}_{s, a, s' \sim \mathcal{D}} \left[
							\left( Q_\theta(s, a) - (r(s, a) + \gamma \hat{Q}_\theta(s',
							\textcolor{red}{\pi_\phi (s')})) \right)^2 \right] \]
						</p>
					</div>
					<p class="text-base md:text-lg mb-4">
						To optimize this objective, we only need to sample from the policy.
						For any reasonable \( \color{red} \pi_\phi \), we are going to be
						able to efficiently sample, and therefore train the Critic.
					</p>
					<p class="text-base md:text-lg mb-4">
						The Policy objective will look like:
					</p>
					<div class="overflow-x-auto">
						<p class="font-mono bg-gray-100 p-2 text-base rounded mb-4">
							\[ \mathcal{L}^\pi = - \mathbb{E}_{s \sim \mathcal{D}} \left[
							Q_\theta (s, \textcolor{red}{\pi_\phi (s)}) \right] \]
						</p>
					</div>
					<p class="text-base md:text-lg mb-4">
						Directly optimizing this requires computing \(\nabla_\phi
						\textcolor{red}{\pi_\phi(s)}\), or how the policy outputs change as
						we vary the inputs. This quantity is undefined for auto-regressive
						transformers, because their outputs are discrete.
					</p>
					<div class="flex justify-center my-4">
						<video loop muted data-autoplay class="w-full max-w-sm h-auto my-4">
							<source
								data-src="./data/autoregressive_transformer_animation.mp4"
								type="video/mp4"
							/>
						</video>
					</div>
					<p class="text-base md:text-lg mb-4">
						Further, calculating it for diffusion policies requires
						back-propagating through the denoising chain, which is a slow and
						unstable process.
					</p>
					<div class="flex justify-center my-4">
						<video loop muted data-autoplay class="w-full max-w-sm h-auto my-4">
							<source
								data-src="./data/diffusion_animation.mp4"
								type="video/mp4"
							/>
						</video>
					</div>
					<p class="text-base md:text-lg mb-4 font-semibold">
						<b>Takeaway:</b> the <i>Policy Improvement operator</i> is the
						bottleneck for improving arbitrary policies.
					</p>
				</div>
			</div>
			<div class="section">
				<div class="method"></div>
			</div>
			<div class="section">
				<div class="results">Section 4</div>
			</div>
			<div class="section">
				<div class="footer">
					Attribution:
					<a
						href="https://www.flaticon.com/free-icons/neural-network"
						title="neural-network icons"
						>Neural-network icons created by Freepik - Flaticon</a
					>
				</div>
			</div>
		</div>

		<script src="https://cdnjs.cloudflare.com/ajax/libs/fullPage.js/4.0.20/fullpage.min.js"></script>
		<script src="script.js"></script>
		<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</body>
</html>
